{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "os.chdir(f\"{path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extraction\n",
    "\n",
    "### If 'result' DataFrame needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data structure\n",
    "# .\n",
    "# ├── aps-dataset-metadata-2013\n",
    "# │   └── PR\n",
    "# │   └── PRA\n",
    "# │   └── PRB\n",
    "# │   ...\n",
    "# │   └── RMP\n",
    "# ├── json_to_csv.py\n",
    "# ├── result_PR1.csv\n",
    "# ├── result_PR2.csv\n",
    "# ├── ...\n",
    "# ├── result_RMP1.csv\n",
    "# └── result.csv\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def json_to_df(json_data): # json file to DataFrame in a particular format\n",
    "    \n",
    "    json_cols = json_data.keys()\n",
    "    result = dict()\n",
    "    \n",
    "    if \"id\" in json_cols:\n",
    "        result[\"doi\"] = json_data[\"id\"]\n",
    "    else:\n",
    "        result[\"doi\"] = [\"\"]\n",
    "    \n",
    "    if \"authors\" in json_cols:\n",
    "        numAuthor = len(json_data[\"authors\"])\n",
    "        names = []\n",
    "        surnames = []\n",
    "        \n",
    "        for author in json_data[\"authors\"]:\n",
    "            if \"name\" in author.keys():\n",
    "                names.append(author[\"name\"].lower().replace(\" \", \"\"))\n",
    "            else:\n",
    "                names.append(\"\")\n",
    "            \n",
    "            if \"surname\" in author.keys():\n",
    "                surnames.append(author[\"surname\"].lower())\n",
    "        \n",
    "        result[\"name\"] = names\n",
    "        result[\"order\"] = list(range(1, numAuthor + 1))\n",
    "        result[\"numAuthor\"] = numAuthor\n",
    "        \n",
    "        # alphabetical order\n",
    "        if numAuthor >= 4 and surnames == sorted(surnames):\n",
    "            result[\"is_alpha\"] = True\n",
    "        else:\n",
    "            result[\"is_alpha\"] = False\n",
    "    else:\n",
    "        result[\"name\"] = [\"\"]\n",
    "        result[\"order\"] = [\"\"]\n",
    "        result[\"numAuthor\"] = 0\n",
    "        result[\"is_alpha\"] = False\n",
    "    \n",
    "    if \"date\" in json_cols:\n",
    "        result[\"year\"] = json_data[\"date\"][:4]\n",
    "    else:\n",
    "        result[\"year\"] = [\"\"]\n",
    "    \n",
    "    if \"articleType\" in json_cols:\n",
    "        result[\"articleType\"] = json_data[\"articleType\"]\n",
    "    else:\n",
    "        result[\"articleType\"] = [\"\"]\n",
    "    \n",
    "    if \"journal\" in json_cols:\n",
    "        result[\"journal\"] = json_data[\"journal\"][\"id\"]\n",
    "    else:\n",
    "        result[\"journal\"] = [\"\"]\n",
    "    \n",
    "    for i in range(10):\n",
    "        result[\"pacs\" + str(i) + \"0\"] = 0\n",
    "    \n",
    "    if \"classificationSchemes\" in json_cols and \"pacs\" in json_data[\"classificationSchemes\"].keys():\n",
    "        for pac in json_data[\"classificationSchemes\"][\"pacs\"]:\n",
    "            if pac[\"id\"][0] in list(map(str, range(10))):\n",
    "                result[\"pacs\" + pac[\"id\"][0] + \"0\"] = 1\n",
    "    \n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "def init_df():\n",
    "    \n",
    "    cols = [\"doi\", \"name\", \"order\", \"numAuthor\", \"is_alpha\",\n",
    "            \"year\", \"articleType\", \"journal\"] + [\"pacs\" + str(i) + \"0\" for i in range(10)]\n",
    "    \n",
    "    return pd.DataFrame(columns=cols)\n",
    "\n",
    "# 'aps-dataset-metadata-2013' json files to each dataframe, then to .csv\n",
    "for file_path in glob.glob(\"./aps-dataset-metadata-2013/*\"):\n",
    "    dir_name = file_path.split(\"/\")[-1]\n",
    "    result = init_df() # dataframe default\n",
    "    \n",
    "    for i, file_name in tqdm(enumerate(glob.glob(file_path + \"/**\", recursive=True), 1)):\n",
    "        if file_name.endswith(\".json\"):        \n",
    "            with open(file_name, \"r\") as j:\n",
    "                json_data = json.loads(j.read())\n",
    "                result = result.append(json_to_df(json_data), ignore_index=True)\n",
    "                \n",
    "        if i % 10000 == 0:\n",
    "            file_id = dir_name + str(i // 10000)\n",
    "            result.to_csv(f\"./result_{file_id}.csv\", index=False) # dataframe to csv\n",
    "            result = init_df() # dataframe default\n",
    "            \n",
    "    # dataframe to csv (save)\n",
    "    file_id = dir_name + str(i//10000 + 1)\n",
    "    result.to_csv(f\"./result_{file_id}.csv\", index=False)\n",
    "\n",
    "# csvs to one csv\n",
    "result = init_df()\n",
    "\n",
    "for file_name in tqdm(glob.glob(\"./*\")):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        result = pd.concat([result, pd.read_csv(file_name)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If 'surnames' DataFrame needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df_sur(json_data):\n",
    "    \n",
    "    json_cols = json_data.keys()\n",
    "    surs = dict()\n",
    "    \n",
    "    if \"authors\" in json_cols:\n",
    "        names = []\n",
    "        surnames = []\n",
    "        \n",
    "        for author in json_data[\"authors\"]:\n",
    "            if \"name\" in author.keys():\n",
    "                names.append(author[\"name\"].lower().replace(\" \", \"\"))\n",
    "            else:\n",
    "                names.append(\"\")\n",
    "            \n",
    "            if \"surname\" in author.keys():\n",
    "                surnames.append(author[\"surname\"].lower())\n",
    "            else:\n",
    "                surnames.append(\"\")\n",
    "        \n",
    "        surs[\"name\"] = names\n",
    "        surs[\"surname\"] = surnames\n",
    "    else:\n",
    "        surs[\"name\"] = [\"\"]\n",
    "        surs[\"surname\"] = [\"\"]\n",
    "\n",
    "    return pd.DataFrame(surs)\n",
    "\n",
    "def init_df_sur():\n",
    "    \n",
    "    cols = [\"name\", \"surname\"]\n",
    "    \n",
    "    return pd.DataFrame(columns=cols)\n",
    "\n",
    "for file_path in glob.glob(\"./aps-dataset-metadata-2013/*\"):\n",
    "    dir_name = file_path.split(\"/\")[-1]\n",
    "    surs = init_df_sur()\n",
    "    \n",
    "    for i, file_name in tqdm(enumerate(glob.glob(file_path + \"/**\", recursive=True), 1)):\n",
    "        if file_name.endswith(\".json\"):        \n",
    "            with open(file_name, \"r\") as j:\n",
    "                json_data = json.loads(j.read())\n",
    "                surs = surs.append(json_to_df_sur(json_data), ignore_index=True)\n",
    "                \n",
    "        if i % 10000 == 0:\n",
    "            file_id = dir_name + str(i // 10000)\n",
    "            surs.to_csv(f\"./surname_{file_id}.csv\", index=False)\n",
    "            surs = init_df_sur()\n",
    "            \n",
    "    file_id = dir_name + str(i//10000 + 1)\n",
    "    surs.to_csv(f\"./surname_{file_id}.csv\", index=False)\n",
    "\n",
    "surs = init_df_sur()\n",
    "\n",
    "for file_name in tqdm(glob.glob(\"./*\")):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        surs = pd.concat([surs, pd.read_csv(file_name)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: special character handling\n",
    "\n",
    "This part is already performed, so check for the methodology purposes instead of actually running them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "JWcHphPee6Xk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sbc = pd.read_csv(\"./sbc_edited.csv\").sort_values(by = ['author']).reset_index(drop=True)\n",
    "sbc.author = sbc.author.str.lower()\n",
    "\n",
    "gender = pd.read_csv(\"./gender_edited.csv\").sort_values(by = ['Name']).apply(\n",
    "        lambda x: x.astype(str).str.lower()).reset_index(drop=True)\n",
    "\n",
    "result = pd.read_csv(\"./result.csv\").sort_values(\n",
    "    by = ['name']).reset_index(drop=True)\n",
    "result.name = result.name.str.lower()\n",
    "\n",
    "surs = pd.read_csv(\"./surnames.csv\").sort_values(\n",
    "    by = ['name']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example:\n",
    "\n",
    "conv = []\n",
    "\n",
    "for x in range(len(surs)):\n",
    "    try:\n",
    "        if not surs['surname'][x].isascii():\n",
    "            for c in surs['surname'][x]:\n",
    "                if not c.isascii():\n",
    "                    conv.append(c)\n",
    "    except AttributeError:\n",
    "        continue\n",
    "\n",
    "for x in range(len(surs)):\n",
    "    try:\n",
    "        if not surs['surname'][x].isalpha():\n",
    "            for c in surs['surname'][x]:\n",
    "                if not c.isalpha():\n",
    "                    conv.append(c)\n",
    "    except AttributeError:\n",
    "        continue\n",
    "\n",
    "conv = sorted(list(set(conv)))\n",
    "\n",
    "# change target to appropriate characters\n",
    "target = [''] * len(conv)\n",
    "\n",
    "for s, t in zip(conv, target):\n",
    "    surs['surname'] = surs['surname'].str.replace(s, t)\n",
    "    print(f'replaced: {s} to {t}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Name matching\n",
    "\n",
    "### Fuzzymatching (not recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install fuzzymatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This package is a bit more optimized than the fuzzywuzzy package. The reason I do not recommend this methodology, however, is that it returns minimal enhancement in regards to matching, yet the time cost and the memory cost are extremely demanding. I write only a partial documentation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = fuzzymatcher.fuzzy_left_join(sbc, gender, left_on = \"author\", right_on = \"Name\")\n",
    "sbc_fuzz = pd.merge(sbc, f, on=['author', 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = fuzzymatcher.fuzzy_left_join(gender, sbc, left_on = \"Name\", right_on = \"author\")\n",
    "g_fuzz = pd.merge(gender, f2, on=['Name', 'Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best match score threshold to 0.25\n",
    "\n",
    "sbc_fuzz = sbc_fuzz[sbc_fuzz['best_match_score'] >= 0.25].sort_values('best_match_score').drop(\n",
    "    ['__id_left','__id_right'], axis=1).reset_index(drop=True)\n",
    "g_fuzz = g_fuzz[g_fuzz['best_match_score'] >= 0.25].sort_values('best_match_score').drop(\n",
    "    ['__id_left','__id_right'], axis=1).reset_index(drop=True)\n",
    "\n",
    "fuzzy = sbc_fuzz.append(g_fuzz).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_left = fuzzy.drop(['Name'], axis=1)\n",
    "fuzzy_left.rename(columns={'Gender': 'gender'}, inplace=True)\n",
    "fuzzy_right = fuzzy.drop(['author'], axis=1)\n",
    "fuzzy_right.rename(columns={'Name': 'author', 'Gender': 'gender'}, inplace=True)\n",
    "\n",
    "nameinfo = fuzzy_left.append(fuzzy_right).reset_index(drop=True)\n",
    "nameinfo.id = nameinfo.id.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surname matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNXs/Kqp8m75qLbgNriQJth",
   "collapsed_sections": [],
   "name": "data_processing_to_samuel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
