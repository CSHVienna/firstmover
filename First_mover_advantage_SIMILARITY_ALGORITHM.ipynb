{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "First-mover advantage SIMILARITY ALGORITHM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNMaUr85up0EQ4QoUuK6ltz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/therobinkay/firstmover/blob/main/First_mover_advantage_SIMILARITY_ALGORITHM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR0Omp2E6i0G",
        "outputId": "38cdf303-00e4-4d99-aea7-375b3a5bdaee"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYxtBAq36kRT"
      },
      "source": [
        "!cd \"drive/My Drive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlI3P-Dl6eCF"
      },
      "source": [
        "# import all necessary packages\n",
        "\n",
        "import pandas as pd\n",
        "import scipy.stats\n",
        "from collections import defaultdict\n",
        "from itertools import combinations as comb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9vifqLV6oe0"
      },
      "source": [
        "# read all necessary .csv files\n",
        "\n",
        "cdata = pd.read_csv('drive/My Drive/codes/cdata.csv')\n",
        "\n",
        "cb = pd.read_csv(\"drive/My Drive/codes/citationBara.csv\")\n",
        "# extra modification for this analysis\n",
        "cb['pair'] = \"(\" + cb[\"citing_doi\"] + \", \" + cb[\"cited_doi\"] + \")\"\n",
        "cb['pair2'] = \"(\" + cb[\"cited_doi\"] + \", \" + cb[\"citing_doi\"] + \")\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwKtuoSs6twd"
      },
      "source": [
        "# Similarity Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZvy5sBj6qYj"
      },
      "source": [
        "# Step 00a: Creating a DataFrame:\n",
        "# p(doi, id, gender, year, PACS),\n",
        "# r(doi, id, gender, year, PACS)\n",
        "hdata = cdata.query('citing_order == 1 & cited_order == 1\\\n",
        " & citing_is_alpha == False & cited_is_alpha == False').drop(['citing_order',\n",
        "  'cited_order', 'citing_numAuthor', 'cited_numAuthor', 'citing_is_last',\n",
        "  'cited_is_last', 'citing_is_alpha', 'cited_is_alpha', 'cited_year',\n",
        "  'citing_articleType', 'cited_articleType', 'citing_journal', 'cited_journal',\n",
        "  'citing_exceptions', 'cited_exceptions'],axis=1)\n",
        "\n",
        "# print(len(hdata))\n",
        "# print(hdata['citing_doi'].nunique())\n",
        "\n",
        "# hdata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB9avBBv6wc7"
      },
      "source": [
        "# Step 00b: Creating a DataFrame: paper, gender, year\n",
        "\n",
        "refsim = hdata[['citing_doi', 'citing_gender', 'citing_year']].drop_duplicates(\n",
        "    subset = 'citing_doi')\n",
        "refsim.columns = [\"paper\", \"gender\", \"year\"]\n",
        "\n",
        "# print(len(refsim))\n",
        "# refsim.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqkAMiMB6xob"
      },
      "source": [
        "# Step 01: Creating a DataFrame: paper, reference, count (of reference)\n",
        "\n",
        "N = 1 #enter sim number\n",
        "for N in range(N+1):\n",
        "  cited = 'cited_'+str(N)\n",
        "  citing = 'citing_'+str(N)\n",
        "  \n",
        "  sim = hdata[hdata[cited] == True]\n",
        "  sim = sim[sim[citing] == True]\n",
        "  sim['count'] = sim.groupby('cited_doi')['cited_doi'].transform('count')\n",
        "  sim.sort_values(\"count\", axis = 0, ascending = True, inplace = True,\n",
        "                  na_position ='first')\n",
        "  sim = sim[['citing_doi', 'cited_doi', 'count']]\n",
        "  sim.columns = [\"paper\", \"reference\", \"count\"]\n",
        "\n",
        "# print(len(sim))\n",
        "# sim.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONhGmhA-6y9c"
      },
      "source": [
        "# Step 02: Creating a dictionary of 'sim's by count\n",
        "\n",
        "sim_x={key:sim[sim['count']==key] for key in sim['count'].unique()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQdbA-LM68_8"
      },
      "source": [
        "# Step 03: Link papers with connections together as groups\n",
        "\n",
        "sim_y = {}\n",
        "nunique_list = {}\n",
        "for key, sim_x_sub in sim_x.items():\n",
        "  cbt = sim_x_sub\n",
        "  from_doi = cbt[\"paper\"]\n",
        "  to_doi = cbt[\"reference\"]\n",
        "\n",
        "  nunique_list[key] = len(to_doi.unique())\n",
        "\n",
        "  parent = {}\n",
        "  for ref in to_doi.unique():\n",
        "    parent[ref] = []\n",
        "  for a, b in zip(from_doi, to_doi):\n",
        "    parent[b].append(a)\n",
        "\n",
        "  pair_count = {}\n",
        "  for l in parent.values():\n",
        "    for tp in comb(sorted(l), 2):\n",
        "      if tp in pair_count:\n",
        "        pair_count[tp] += 1\n",
        "      else:\n",
        "        pair_count[tp] = 1\n",
        "  dfc = []\n",
        "  for p, value in pair_count.items():\n",
        "    dfc.append([p[0], p[1], value])\n",
        "  dfc = pd.DataFrame(dfc, columns=['paper1', 'paper2', 'common'])\n",
        "  sim_y[key] = dfc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNnEyWVZ6-Ds"
      },
      "source": [
        "# Step 04: Creating a dictionary: paper1, paper2, reference, common, freq1, freq2\n",
        "\n",
        "freq = {}\n",
        "\n",
        "for key, tables in sim_y.items():\n",
        "  ftable = sim_x[key].groupby([\"paper\"]).size().reset_index(name='frequency')\n",
        "  freq[key] = ftable\n",
        "\n",
        "fin = {}\n",
        "for key, tables in sim_y.items():\n",
        "  if len(tables) > 0:\n",
        "    tcount = freq[key]\n",
        "    final_table = tables.merge(tcount, left_on='paper1', right_on='paper')\n",
        "    final_table = final_table.merge(tcount, left_on='paper2', right_on='paper')\n",
        "    final_table = final_table.drop(['paper_x', 'paper_y'], axis=1)\n",
        "    \n",
        "    fin[key] = final_table\n",
        "\n",
        "# fin[9].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L762ydE26_uc"
      },
      "source": [
        "# Step 05: Assigning q-values to all pairs\n",
        "\n",
        "H_func = scipy.stats.hypergeom.pmf\n",
        "\n",
        "for key, table in fin.items():\n",
        "  qval = []\n",
        "  _dp = {}\n",
        "  nbk = nunique_list[key]\n",
        "\n",
        "  for i, r in table.iterrows():\n",
        "    Hs = []\n",
        "    di, dj = r['frequency_x'], r['frequency_y']\n",
        "    nijk = r['common']\n",
        "\n",
        "    for X in range(nijk):\n",
        "      _ref = (X, nbk, di, dj)\n",
        "      if _ref in _dp:\n",
        "        H = _dp[_ref]\n",
        "      else:\n",
        "        H = H_func(X, nbk, di, dj)\n",
        "        _dp[_ref] = H\n",
        "      Hs.append(H)\n",
        "\n",
        "    qval.append(1-sum(Hs))\n",
        "\n",
        "  fin[key]['qval'] = qval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6CDY-4O7BEE"
      },
      "source": [
        "# Step 06: Creating a DataFrame: paper1, paper2, common, freq1, freq2, qval\n",
        "\n",
        "df = pd.concat(fin.values())\n",
        "df.groupby(['paper1', 'paper2']).size()\n",
        "df = df.sort_values(['paper1', 'paper2'])\n",
        "# print(len(df))\n",
        "# df.sort_values('common', ascending=False).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AqMTJ_N7CYM"
      },
      "source": [
        "# Step 07: Combining equivalent pairs together with the minimum qval\n",
        "\n",
        "df = df.drop([\"common\", \"frequency_x\", \"frequency_y\"], axis=1).sort_values(\n",
        "    [\"paper1\", \"paper2\", \"qval\"]).reset_index(drop=True).groupby(\n",
        "        [\"paper1\", \"paper2\"], as_index=False).min()\n",
        "\n",
        "# print(len(df))\n",
        "# df.sort_values('qval').head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHjH755C7Dqs"
      },
      "source": [
        "# Step 08: Assigning k to all pairs\n",
        "\n",
        "pairs = \"(\" + df[\"paper1\"] + \", \" + df[\"paper2\"] + \")\"\n",
        "df['k'] = pairs.isin(cb.pair) | pairs.isin(cb.pair2)\n",
        "\n",
        "# print(len(df))\n",
        "# print(sum(df['k']))\n",
        "# df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZItFJew7E-M"
      },
      "source": [
        "# Step 09: Merging a DataFrame: adding gender & year\n",
        "\n",
        "df = df.merge(refsim, left_on='paper1', right_on='paper', how='inner').drop(\n",
        "    ['paper'], axis=1).rename({'gender': 'gender1', 'year': 'year1'}, axis=1)\n",
        "df = df.merge(refsim, left_on='paper2', right_on='paper', how='inner').drop(\n",
        "    ['paper'], axis=1).rename({'gender': 'gender2', 'year': 'year2'}, axis=1)\n",
        "\n",
        "cols = ['paper1', 'gender1', 'year1', 'paper2', 'gender2', 'year2', 'qval', 'k']\n",
        "df = df.reindex(columns=cols)\n",
        "\n",
        "# print(len(df))\n",
        "# df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5w5ephF7G7t"
      },
      "source": [
        "# Step XX: Converting to .csv file\n",
        "\n",
        "# df.to_csv(r'drive/My Drive/codes/sim1.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}